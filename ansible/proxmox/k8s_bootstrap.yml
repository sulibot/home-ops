---
- name: Initialize the first control plane node if not already done
  hosts: "{{ groups['kube_control_plane'][0] }}"
  become: true
  tasks:
    - name: Check if kubeadm init has already been run
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeadm_init_check

    - name: Run kubeadm init with config file (only if not initialized)
      command: kubeadm init --config /root/kubeadm-config.yaml
      when: not kubeadm_init_check.stat.exists

    - name: Setup kubectl config for root (only if not already set)
      shell: |
        mkdir -p /root/.kube
        cp /etc/kubernetes/admin.conf /root/.kube/config
        chown root:root /root/.kube/config
      when: not kubeadm_init_check.stat.exists

    - name: Generate the control plane join command with token and certificate key
      shell: |
        kubeadm token create --print-join-command --certificate-key $(kubeadm init phase upload-certs --upload-certs | grep -vw -e certificate -e Namespace)
      register: control_plane_join_command

    - name: Generate the worker node join command (with token and discovery hash)
      shell: |
        kubeadm token create --print-join-command
      register: worker_node_join_command

    - name: Set the join commands as facts
      set_fact:
        global_control_plane_join_command: "{{ control_plane_join_command.stdout }} --ignore-preflight-errors=all"
        global_worker_join_command: "{{ worker_node_join_command.stdout }} --ignore-preflight-errors=all"

    - name: Copy kube admin.conf to the local controller
      fetch:
        src: /etc/kubernetes/admin.conf
        dest: "{{ lookup('env', 'HOME') }}/.kube/config"
        flat: true
        validate_checksum: false

    - name: Fix kube-vip.yaml path on the first control plane node
      shell: |
        sed -i '/hostPath:/!b;n;s|path: /etc/kubernetes/super-admin.conf|path: /etc/kubernetes/admin.conf|' /etc/kubernetes/manifests/kube-vip.yaml

    - name: Check if any Cilium resource exists in kube-system
      shell: kubectl get all -n kube-system | grep -i cilium || true
      register: cilium_check
      failed_when: false

    - name: Install Cilium using Helm (if not already installed)
      shell: |
        helm repo add cilium https://helm.cilium.io || true
        helm repo update
        helm upgrade --install cilium cilium/cilium --version 1.16.5 --namespace kube-system --create-namespace -f /root/cilium-values.yaml
        kubectl -n kube-system rollout restart deployment/cilium-operator || true
        kubectl -n kube-system rollout restart daemonset/cilium || true
      when: cilium_check.stdout == ""

- name: Join remaining control plane nodes if not already joined
  hosts: "{{ groups['kube_control_plane'][1:] }}"
  become: true
  tasks:
    - name: Check if this control plane node is already part of the cluster
      shell: kubectl get nodes --no-headers | grep -w $(hostname)
      register: control_plane_join_check
      failed_when: false

    - name: Check for existing kubelet config to detect partial join
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_config_check

    - name: Debug the join status
      debug:
        msg: >
          Skipping join for {{ inventory_hostname }} because it is either fully joined (found in kubectl get nodes)
          or partially joined (existing kubelet config).
      when: control_plane_join_check.rc == 0 or kubelet_config_check.stat.exists

    - name: Join control plane nodes (only if not already joined)
      shell: "{{ hostvars[groups['kube_control_plane'][0]]['global_control_plane_join_command'] }}"
      when: control_plane_join_check.rc != 0 and not kubelet_config_check.stat.exists

    - name: Fix kube-vip.yaml path on remaining control plane nodes
      shell: |
        sed -i '/hostPath:/!b;n;s|path: /etc/kubernetes/super-admin.conf|path: /etc/kubernetes/admin.conf|' /etc/kubernetes/manifests/kube-vip.yaml

- name: Join worker nodes to the cluster if not already joined
  hosts: kube_worker
  become: true
  tasks:
    - name: Check if this worker node is already part of the cluster
      shell: kubectl get nodes --no-headers | grep -w $(hostname)
      register: worker_node_join_check
      failed_when: false

    - name: Check for existing kubelet config to detect partial join
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_config_check

    - name: Debug the join status
      debug:
        msg: >
          Skipping join for {{ inventory_hostname }} because it is either fully joined (found in kubectl get nodes)
          or partially joined (existing kubelet config).
      when: worker_node_join_check.rc == 0 or kubelet_config_check.stat.exists

    - name: Join worker nodes (only if not already joined)
      shell: "{{ hostvars[groups['kube_control_plane'][0]]['global_worker_join_command'] }}"
      when: worker_node_join_check.rc != 0 and not kubelet_config_check.stat.exists


